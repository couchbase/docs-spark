= Capella Columnar Support
:page-status: Developer Preview
:nav-title: Columnar Support


[abstract]
Connecting to Capella Columnar is very similar to connecting to any Couchbase cluster. This section explains how.

Capella Columnar clusters can be connected to using either Scala or PySpark.
To get bootstrapped, users should view the xref:getting-started.adoc[Scala getting started] or xref:pyspark.adoc[PySpark getting started] guides.

NOTE: Developer Preview mode ("Preview Mode"), provides early access to features which may become generally available ("GA") in future releases and enables you to play with these features to get a sense of how they work.
Preview Mode features and their use are subject to Couchbase's "Non-GA Offering Supplemental Terms", set forth in the License Agreement.
Preview Mode features may not be functionally complete and are not intended for production use.
They are intended for development and testing purposes only.

== Spark Configuration

The first step as usual is to create a `SparkSession`, here connecting to your Capella Columnar cluster.
This works just like connecting to any other type of Couchbase cluster.

[{tabs}]
====
Scala::
+
Change the cluster configuration options to match your own.
--
[source,scala]
----
include::example$Columnar.scala[tag=init,indent=0]
----
--

PySpark::
+
Change the cluster configuration options to match your own, and provide the location of the Spark Connector library if needed (see the xref:pyspark.adoc[PySpark getting started] guide for more details).
--
[source,python]
----
include::example$Columnar.py[tag=init,indent=0]
----
--
====

The following examples will use the `travel-sample` example set of data, which can be loaded through the UI.


== Reading a Dataframe

Let's start by reading a Spark DataFrame from the `airline` collection, which is in the `inventory` scope of the `travel-sample` database:


[{tabs}]
====
Scala::
+
--
[source,scala]
----
include::example$Columnar.scala[tag=reading-dataframe,indent=0]
----
--

PySpark::
+
--
[source,python]
----
include::example$Columnar.py[tag=reading-dataframe,indent=0]
----
--
====



This is a normal Spark DataFrame that we can count, iterate and so on.

[{tabs}]
====
Scala::
+
--
[source,scala]
----
include::example$Columnar.scala[tag=processing-dataframe,indent=0]
----
--

PySpark::
+
--
[source,python]
----
include::example$Columnar.py[tag=processing-dataframe,indent=0]
----
--
====



== Reading a Dataset
(Supported in Scala only, as Apache Spark does not support Datasets via PySpark.)

It can be preferable to read into a Spark `Dataset` rather than a DataFrame, as this lets us use Scala case classes directly.

To do this, we:

. Create an `Airline` case class that matches our expected results.
. Import the `SparkSession` implicits allowing Spark to convert directly to our `Airline` class.
. Do `.as[Airline]` to turn our DataFrame into a `Dataset`.

[source,scala]
----
include::example$Columnar.scala[tag=reading-dataset,indent=0]
----


== Spark SQL

We can use Spark's `createOrReplaceTempView` to create a temporary view from a DataFrame, which we can then run Spark SQL on (which creates another DataFrame):


[{tabs}]
====
Scala::
+
--
[source,scala]
----
include::example$Columnar.scala[tag=sql,indent=0]
----
--

PySpark::
+
--
[source,python]
----
include::example$Columnar.py[tag=sql,indent=0]
----
--
====

Note this SQL is executed purely within Spark, and is not sent to the Capella Columnar cluster.

