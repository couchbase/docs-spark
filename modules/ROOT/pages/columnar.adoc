= Capella Columnar Support
:page-status: Developer Preview
:nav-title: Columnar Support


[abstract]
Connecting to Capella Columnar is very similar to connecting to any Couchbase cluster over an encrypted connection. This section explains how.

NOTE: Capella Columnar support is currently at an xref:index.adoc#interface-stability[Uncommitted] level in the Couchbase Spark Connector.
We encourage users to try it out and provide feedback, but there is a possibility that changes to the API may be required.


== Spark Configuration

The first step as usual is to create a `SparkSession`, here connecting to your Capella Columnar cluster.
This works just like connecting to any other type of Couchbase cluster.

[source,scala]
----
include::example$Columnar.scala[tag=init,indent=0]
----

The following examples will use the `travel-sample` example set of data, which can be loaded through the UI.


== Reading a Dataframe

Let's start by reading a Spark DataFrame from the `airline` collection, which is in the `inventory` scope of the `travel-sample` database:

[source,scala]
----
include::example$Columnar.scala[tag=reading-dataframe,indent=0]
----

This is a normal Spark DataFrame that we can count, iterate and so on.

[source,scala]
----
include::example$Columnar.scala[tag=processing-dataframe,indent=0]
----


== Reading a Dataset

It can be preferable to read into a Spark `Dataset` rather than a DataFrame, as this lets us use Scala case classes directly.

To do this, we:

. Create an `Airline` case class that matches our expected results.
. Import the `SparkSession` implicits allowing Spark to convert directly to our `Airline` class.
. Do `.as[Airline]` to turn our DataFrame into a `Dataset`.

[source,scala]
----
include::example$Columnar.scala[tag=reading-dataset,indent=0]
----


== Spark SQL

We can use Spark's `createOrReplaceTempView` to create a temporary view from a DataFrame, which we can then run Spark SQL on (which creates another DataFrame):

[source,scala]
----
include::example$Columnar.scala[tag=sql,indent=0]
----

Note this SQL is executed purely within Spark, and is not sent to the Capella Columnar cluster.


== PySpark

Capella Columnar can be accessed through PySpark.
See our xref:pyspark.adoc[PySpark documentation] for more details.

[source,python]
----
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Couchbase Spark Connector Columnar Example") \
    .config("spark.couchbase.connectionString", "couchbases://your-columnar-endpoint.cloud.couchbase.com.com") \
    .config("spark.couchbase.username", "username") \
    .config("spark.couchbase.password", "password") \
    .getOrCreate()

df = (spark.read.format("couchbase.columnar")
      .option("database", "travel-sample")
      .option("scope", "inventory")
      .option("collection", "airline")
      .load())
df.show()
----
